<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Domain Adaptation for Generalizable Imitation from Egocentric Human Data">
  <meta name="keywords" content="EgoBridge, EgoMimic, Bridge, OT, Transport, Imitation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EgoBridge | Domain Adaptation for Generalizable Imitation from Egocentric Human Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.rpunamiya.dev">Ryan Punamiya</a>,
            </span>
            <span class="author-block">
              <a href="https://dhruv2012.github.io">Dhruv Patel</a>,  
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/elmo-aphiwetsa/">Elmo Aphiwetsa</a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/pranav-kuppili-b05071250">Pranav Kuppili</a>,
            </span>
            <span class="author-block">
              <a href="https://lawrencez22.github.io">Lawrence Y. Zhu</a>,
            </span>
            <br><span class="author-block">
              <a href="https://simarkareer.com">Simar Kareer</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://jhoffman.github.io/">Judy Hoffman</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~danfei">Danfei Xu</a><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Georgia Institute of Technology</span>
            <p class="is-size-6 has-text-grey">
              <sup>*</sup> equal advising
            </p> 
          </div>

          <div class="column has-text-centered" style="margin-top: 1rem;">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.19626" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code Release</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://x.com/ryan_punamiya/status/1971239110439469256" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Tweet</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
<body>

  <div style="background-color: #fff0f0; color: red; font-weight: bold; text-align: center; padding: 10px; font-size: 1.2em;">
    EgoBridge will be featured as an Oral presentation at the 
    <a href="https://sites.google.com/view/h2r-corl2025/home" style="color: rgb(119, 0, 255); text-decoration: underline;">
      H2R Workshop</a>. Come see us there!
  </div>  
  
<section class="section">
  <div class="container is-max-widescreen">
        <div class="publication-image">
          <img src="./static/figures/teaser_figure.png" 
               alt="EgoBridge Teaser" 
               style="width: 600%; height: auto;"/>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 125%">
            Egocentric human experience data presents a vast resource for scaling up end-to-end imitation learning for robotic manipulation. However, significant domain gaps in visual appearance, sensor modalities, and kinematics between human and robot impede knowledge transfer. This paper presents EgoBridge, a unified co-training framework that explicitly aligns the policy latent spaces between human and robot data using domain adaptation. Through a measure of discrepancy on the joint policy latent features and actions based on Optimal Transport (OT), we learn observation representations that not only align between the human and robot domain but also preserve the action-relevant information critical for policy learning. EgoBridge achieves a significant absolute policy success rate improvement by 44% over human-augmented cross-embodiment baselines in three real-world single-arm and bimanual manipulation tasks. EgoBridge also generalizes to new objects, scenes, and tasks seen only in human data, where baselines fail entirely.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="rows is-centered">
        <div class="row is-full-width">

          <!-- Optimal Transport -->
          <h2 class="title is-3 has-text-centered"><span class="dvima">Latent Alignment with Joint Optimal Transport</span></h2>
          <br>
          <video poster="" autoplay muted controls loop height="100%">
            <source src="static/videos/tweet3.mp4" type="video/mp4">
          </video>
          <br>
          <span style="font-size: 110%">
            <b>Latent alignment with OT</b>. Various domain gaps limit transfer between human and robot. 
            Our key insight: leverage inherent human ↔ robot motion similarities to supervise latent alignment. 
            We formalize this as an <b>Optimal Transport</b> problem, probabilistically mapping human + robot data.
          </span>

          <br><br>

          <!-- DTW -->
          <h2 class="title is-3 has-text-centered"><span class="dvima">Dynamic Time Warping (DTW)</span></h2>
          <br>
          <video poster="" autoplay muted controls loop height="100%">
            <source src="static/videos/tweet4.mp4" type="video/mp4">
          </video>
          <br>
          <span style="font-size: 110%">
            <b>Joint Latent-Action Cost with OT</b>. To integrate action information into OT, we leverage 
            <b>Dynamic Time Warping (DTW)</b>. In a batch, DTW finds the most behaviorally similar human 
            samples to a given sample. These "pseudo-pairs" are used to discount the latent cost in OT, 
            aligning the joint latent–action distribution.
          </span>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="rows is-centered">
        <div class="row is-full-width">
          <h2 class="title is-3 has-text-centered"><span class="dvima">Architecture</span></h2>
          <br>
          <video poster="" autoplay muted controls loop height="100%">
            <source src="static/videos/tweet5.mp4" type="video/mp4">
          </video>
          <br>
          <span style="font-size: 110%">
            <b>Unified Policy</b>. Our method consists of a simple unified policy which integrates directly with human-robot co-training.  
            (a) We embed human and robot samples into a shared latent space with encoder <b>f<sub>φ</sub></b>.  
            (b) We leverage OT as an auxiliary loss function to encourage the encoder <b>f<sub>φ</sub></b> to align human and robot latents.  
            (c) We co-train the policy on human and robot data by jointly predicting actions and computing a combined BC loss.
          </span>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="rows is-centered">
        <div class="row is-full-width">

          <!-- ===================== RESULTS HEADER ===================== -->
          <h2 class="title is-3 has-text-centered"><span class="dvima">Results</span></h2>
          <br>

          <!-- ===================== In-domain Performance ===================== -->
          <h3 class="title is-4 has-text-centered"><span class="dvima">Experiments: In-domain Performance</span></h3>
          <br>
          <div class="results-carousel" style="display: flex; flex-direction: column; align-items: center;">

            <!-- Video items -->
            <div class="item active" data-description="<b>(a) Laundry:</b> the robot folds a polo shirt placed randomly on the table.">
              <video autoplay muted loop>
                <source src="static/videos/laundry.mp4" type="video/mp4">
              </video>
            </div>
          
            <div class="item" data-description="<b>(b) Scoop Coffee:</b> the robot picks up a scoop filled with coffee beans and empties it into the target container placed randomly.">
              <video autoplay muted loop>
                <source src="static/videos/scoop.mp4" type="video/mp4">
              </video>
            </div>
          
            <div class="item" data-description="<b>(c) Drawers:</b> the robot picks up a toy and places it into the open drawer and closes it.">
              <video autoplay muted loop>
                <source src="static/videos/drawers.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Description -->
            <div id="video-description" style="font-size: 110%; text-align: center; margin-top: 15px;">
              <b>(a) Laundry:</b> the robot folds a polo shirt placed randomly on the table.
            </div>

            <!-- Dots -->
            <div class="carousel-dots">
              <span class="dot" data-index="0"></span>
              <span class="dot" data-index="1"></span>
              <span class="dot" data-index="2"></span>
            </div>

            <!-- Prev / Next -->
            <div class="carousel-controls" style="margin-top: 10px;">
              <button id="prev-btn" class="carousel-nav prev-button">
                <span class="icon is-large"><i class="fas fa-chevron-left"></i></span>
              </button>
              <button id="next-btn" class="carousel-nav next-button">
                <span class="icon is-large"><i class="fas fa-chevron-right"></i></span>
              </button>
            </div>
          </div>

          <!-- In-distribution Performance image + caption -->
          <div class="has-text-centered" style="margin-top: 2rem;">
            <img src="./static/figures/in_dist.png"
                 alt="In-distribution performance figure"
                 style="width: 80%; height: auto;" />
            <div style="font-size:110%; margin-top: 0.6rem;">
              <b>EgoBridge outperforms Robot-BC and other human-augmented baselines by 44% in three real-world tasks.</b>
            </div>
          </div>

          <br><br>

          <!-- ===================== Scene Generalization ===================== -->
          <h3 class="title is-4 has-text-centered"><span class="dvima">Experiments: Scene Generalization</span></h3>
          <br>
          <div class="has-text-centered">
            <video poster="" autoplay muted controls loop style="width: 80%; height: auto;">
              <source src="./static/videos/tweet6.mp4" type="video/mp4">
            </video>
          </div>
          <div class="has-text-centered" style="margin-top: 1rem;">
            <img src="./static/figures/scenegen.png"
                 alt="Scene generalization results figure"
                 style="width: 80%; height: auto;" />
            <div style="font-size:110%; margin-top: 0.6rem;">
              <b>EgoBridge generalizes to an entirely new scene and object combination unseen in robot data where human-augmented baselines fail entirely in the Scoop Coffee task.</b>
            </div>
          </div>

          <br><br>

          <!-- ===================== Behavior Generalization ===================== -->
          <h3 class="title is-4 has-text-centered"><span class="dvima">Experiments: Behavior Generalization</span></h3>
          <br>
          <div class="has-text-centered">
            <video poster="" autoplay muted controls loop style="width: 80%; height: auto;">
              <source src="./static/videos/tweet7.mp4" type="video/mp4">
            </video>
          </div>
          <div style="font-size:110%; text-align:center; margin-top: 0.6rem;">
            <b>EgoBridge allows the robot to generalize to a completely out-of-distribution set of trajectories in the Drawer task where the gripper moving to the top right corner is never observed.</b>
          </div>

          <br><br>

          <!-- ===================== Simulation PushT ===================== -->
          <h3 class="title is-4 has-text-centered"><span class="dvima">Experiments: Simulation PushT</span></h3>
          <br>
          <div class="has-text-centered">
            <img src="./static/figures/pusht_full.png"
                 alt="PushT figure"
                 style="width: 400%; height: auto;" />
          </div>
          <div style="font-size:110%; text-align:center; margin-top: 0.6rem;">
            <b>We simulate a human-robot gap in a reproducible planar pushing task. EgoBridge outperforms standard DA baselines, showing cross-embodiment transfer across new backgrounds and new motions.</b>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="rows is-centered">
        <div class="row is-full-width">
          <h2 class="title is-3 has-text-centered"><span class="dvima">Latent Visualization</span></h2>
          <br>
          <div class="has-text-centered">
            <video poster="" autoplay muted controls loop style="width: 80%; height: auto;">
              <source src="./static/videos/tweet8.mp4" type="video/mp4">
            </video>
          </div>
          <br>
          <span style="font-size: 110%; text-align: center; display: block;">
            <b>Visualization of aligned latents.</b> We visualize the t-SNE of the latent embeddings produced by the encoder and 
            find that <b>EgoBridge</b> not only aligns human–robot distributions but also structures them into shared semantic clusters.
          </span>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="rows is-centered">
        <div class="row is-full-width">
          <h2 class="title is-3 has-text-centered"><span class="dvima">Conclusion</span></h2>
          <p style="font-size: 125%">
            We presented <strong>EgoBridge</strong>, a novel co-training framework designed to enable robots to learn effectively from egocentric human data by explicitly addressing domain gaps. By leveraging Optimal Transport on joint policy latent feature–action distributions, guided by Dynamic Time Warping cost on action trajectories, <strong>EgoBridge</strong> successfully aligns human and robot representations while preserving critical action-relevant information. Our experiments demonstrated significant improvements in real-world task success rates (up to 44% absolute gain) and, importantly, showed robust generalization to novel objects, scenes, and even tasks observed only in human demonstrations, where baselines often failed.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-3"><span class="dvima">BibTeX</span></h2>
          <div class="content has-text-left">
          <pre style="background:#f5f5f5; padding:1em; border-radius:8px; font-size:0.9em; overflow-x:auto;">
@misc{punamiya2025egobridgedomainadaptationgeneralizable,
  title        = {EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data},
  author       = {Ryan Punamiya and Dhruv Patel and Patcharapong Aphiwetsa and 
                  Pranav Kuppili and Lawrence Y. Zhu and Simar Kareer and 
                  Judy Hoffman and Danfei Xu},
  year         = {2025},
  eprint       = {2509.19626},
  archivePrefix= {arXiv},
  primaryClass = {cs.RO},
  url          = {https://arxiv.org/abs/2509.19626}
}
          </pre>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>, <a
              href="https://peract.github.io/">PerAct</a> and <a href="https://mimic-play.github.io">MimicPlay</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

<!-- Custom Script for Button-based Carousel -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    const items = document.querySelectorAll('.item');
    const taskDescriptions = [
      "<b>(a) Laundry:</b> the robot folds a polo shirt placed randomly on the table.",
      "<b>(b) Scoop Coffee:</b> the robot picks up a scoop filled with coffee beans and empties it into the target container placed randomly.",
      "<b>(c) Drawers:</b> the robot picks up a toy and places it into the open drawer and closes it."
    ];

    let currentIndex = 0;
    const taskDescriptionEl = document.getElementById('taskDescription');
    
    // Function to show the active video and hide others
    function showVideo(index) {
      items.forEach((item, i) => {
        if (i === index) {
          item.style.display = 'block';  // Show active item
        } else {
          item.style.display = 'none';   // Hide inactive items
        }
      });
      // Update task description
      taskDescriptionEl.innerHTML = taskDescriptions[index];
    }

    // Button event listeners
    document.getElementById('prevBtn').addEventListener('click', function () {
      currentIndex = (currentIndex - 1 + items.length) % items.length;  // Move backward
      showVideo(currentIndex);
    });

    document.getElementById('nextBtn').addEventListener('click', function () {
      currentIndex = (currentIndex + 1) % items.length;  // Move forward
      showVideo(currentIndex);
    });

    // Initialize the first video
    showVideo(currentIndex);
  });
</script>

<!-- Basic Styling to ensure all videos are controlled and responsive -->
<style>
  .carousel .item {
    display: none;  /* Hide all videos initially */
  }

  .carousel .item video {
    width: 70%;  /* Make the videos smaller */
    max-width: 400px;  /* Set a maximum width */
    height: auto;  /* Maintain aspect ratio */
    margin: 0 auto;  /* Center the video */
  }

  .carousel .item:first-child {
    display: block;  /* Show the first video by default */
  }

  .carousel-nav {
    background-color: #000;
    border: none;
    border-radius: 50%;
    color: #fff;
    padding: 10px;
    font-size: 1.5em;
    cursor: pointer;
  }

  .carousel-nav:hover {
    background-color: #555;
  }

  .icon.is-large {
    font-size: 2em;
  }
</style>
<script>
  // JavaScript for video carousel
  document.addEventListener('DOMContentLoaded', () => {
    const items = document.querySelectorAll('.results-carousel .item');
    const descriptionBox = document.getElementById('video-description');
    let currentIndex = 0;

    // Show the current item and update the description
    function updateCarousel() {
      items.forEach((item, index) => {
        item.classList.toggle('active', index === currentIndex);
      });
      const currentItem = items[currentIndex];
      descriptionBox.innerHTML = `<b>(${String.fromCharCode(97 + currentIndex)})</b> ` + currentItem.dataset.description;
    }

    // Set up the next button
    document.getElementById('next-btn').addEventListener('click', () => {
      currentIndex = (currentIndex + 1) % items.length;
      updateCarousel();
    });

    // Set up the previous button
    document.getElementById('prev-btn').addEventListener('click', () => {
      currentIndex = (currentIndex - 1 + items.length) % items.length;
      updateCarousel();
    });

    // Initialize the carousel
    updateCarousel();
  });

  document.addEventListener('DOMContentLoaded', function () {
    const items = document.querySelectorAll('.item');
    const taskDescriptions = [
      "<b>(a) Laundry:</b> the robot folds a polo shirt placed randomly on the table.",
      "<b>(b) Scoop Coffee:</b> the robot picks up a scoop filled with coffee beans and empties it into the target container placed randomly.",
      "<b>(c) Drawers:</b> the robot picks up a toy and places it into the open drawer and closes it."
    ];
    const dots = document.querySelectorAll('.dot');
    let currentIndex = 0;
    const taskDescriptionEl = document.getElementById('video-description');

    // Function to show the active video and update dots
    function showVideo(index) {
        items.forEach((item, i) => {
            if (i === index) {
                item.style.display = 'block';  // Show active item
            } else {
                item.style.display = 'none';   // Hide inactive items
            }
        });
        // Update task description
        taskDescriptionEl.innerHTML = taskDescriptions[index];
        
        // Update dots
        dots.forEach((dot, i) => {
            if (i === index) {
                dot.classList.add('active');
            } else {
                dot.classList.remove('active');
            }
        });
    }

    // Button event listeners
    document.getElementById('prev-btn').addEventListener('click', function () {
        currentIndex = (currentIndex - 1 + items.length) % items.length;  // Move backward
        showVideo(currentIndex);
    });

    document.getElementById('next-btn').addEventListener('click', function () {
        currentIndex = (currentIndex + 1) % items.length;  // Move forward
        showVideo(currentIndex);
    });

    // Dot event listeners
    dots.forEach(dot => {
        dot.addEventListener('click', function () {
            currentIndex = parseInt(this.getAttribute('data-index'));
            showVideo(currentIndex);
        });
    });

    // Initialize the first video
    showVideo(currentIndex);
});

</script>

</body>
</html>